<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pre-trained Language Model | WestlakeNLP</title><link>https://westlake-nlp.github.io/tag/pre-trained-language-model/</link><atom:link href="https://westlake-nlp.github.io/tag/pre-trained-language-model/index.xml" rel="self" type="application/rss+xml"/><description>Pre-trained Language Model</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://westlake-nlp.github.io/media/icon_huc09762889ea35b5d80a6cd2e52c31adf_1359923_512x512_fill_lanczos_center_3.png</url><title>Pre-trained Language Model</title><link>https://westlake-nlp.github.io/tag/pre-trained-language-model/</link></image><item><title>On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART</title><link>https://westlake-nlp.github.io/publication/2022.-on-the-role-of-pre-trained-language-models-in-word-ordering-a-case-study-with-bart/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://westlake-nlp.github.io/publication/2022.-on-the-role-of-pre-trained-language-models-in-word-ordering-a-case-study-with-bart/</guid><description/></item></channel></rss>