[{"authors":null,"categories":null,"content":"I am a tenured full professor at Westlake University. I worked as associate professor at Westlake University from Sep 2018 to Jun 2022, and assistant professor at Singapore Universit of Technology and Design from Sep 2012 to Aug 2018. Before joining SUTD, I worked as a postdoctoral research associate at University of Cambridge. I received my PhD degree from University of Oxford in Dec 2009, working on statistical Chinese processing for¬†my thesis. I received my MSc degree from University of Oxford in Oct 2006, working on statistical machine translation from Chinese to English by parsing (MSc thesis). I received my undergraduate degree on Computer Science from Tsinghua University, China.\n","date":1906549200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1906549200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a tenured full professor at Westlake University. I worked as associate professor at Westlake University from Sep 2018 to Jun 2022, and assistant professor at Singapore Universit of Technology and Design from Sep 2012 to Aug 2018.","tags":null,"title":"Âº†Â≤≥","type":"authors"},{"authors":["Âº†Â≤≥","Âê≥ÊÅ©ÈÅî"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://westlake-nlp.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["Publication","ACL"],"content":"Intro Regarding the field of artificial intelligence, conferences are more important than journals. According to Google Scholar statistics, the Meeting of the Association for Computational Linguistics (ACL) is the most important publication venue in the field of artificial intelligence - natural language processing and the only CCF-A class conference in this area. The acceptance rate is consistently around 20%.\nWestlakeNLP has accepted 14 long papers to ACL-2023 held in Canada, including 9 papers in the Main track and 5 papers in the Findings track.\n","date":1683590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683590400,"objectID":"221570ef38422c5369b9e10cac109395","permalink":"https://westlake-nlp.github.io/post/acl2023/","publishdate":"2023-05-09T00:00:00Z","relpermalink":"/post/acl2023/","section":"post","summary":"Delighted to share that our group has **9 long papers accepted to the main track of ACL-2023 and 5 long papers accepted to the findings**. The details of these papers will be released soon.üëã","tags":["Artificial Intelligence","Natural Language Processing"],"title":"WestlakeNLP's 14 long papers have been accepted by ACL-2023","type":"post"},{"authors":["Xuefeng Bai","Sen Yang","Leyang Cui","Linfeng Song","Yue Zhang"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"9142082f9262c19530f5e717e9b22cfa","permalink":"https://westlake-nlp.github.io/publication/2022.-cross-domain-generalization-for-amr-parsing/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2022.-cross-domain-generalization-for-amr-parsing/","section":"publication","summary":"Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph from textual input. Recently, there has been notable growth in AMR parsing performance. However, most existing work focuses on improving the performance in the specific domain, ignoring the potential domain dependence of AMR parsing systems. To address this, we extensively evaluate five representative AMR parsers on five domains and analyze challenges to cross-domain AMR parsing. We observe that challenges to cross-domain AMR parsing mainly arise from the distribution shift of words and AMR concepts. Based on our observation, we investigate two approaches to reduce the domain distribution divergence of text and AMR features, respectively. Experimental results on two out-of-domain test sets show the superiority of our method.","tags":["Parsing","AMR"],"title":"Cross-domain Generalization for AMR Parsing","type":"publication"},{"authors":["Yafu Li","Leyang Cui","Yongjing Yin","Yue Zhang"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"9601ada661a0769535f2180eb4ee6044","permalink":"https://westlake-nlp.github.io/publication/2022.-multi-granularity-optimization-for-non-autoregressive-translation/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2022.-multi-granularity-optimization-for-non-autoregressive-translation/","section":"publication","summary":"Despite low latency, non-autoregressive machine translation (NAT) suffers severe performance deterioration due to the naive independence assumption. This assumption is further strengthened by cross-entropy loss, which encourages a strict match between the hypothesis and the reference token by token. To alleviate this issue, we propose multi-granularity optimization for NAT, which collects model behaviours on translation segments of various granularities and integrates feedback for backpropagation. Experiments on four WMT benchmarks show that the proposed method significantly outperforms the baseline models trained with cross-entropy loss, and achieves the best performance on WMT‚Äô16 En‚áîRo and highly competitive results on WMT‚Äô14 En‚áîDe for fully non-autoregressive translation.","tags":["Machine Translation"],"title":"Multi-Granularity Optimization for Non-Autoregressive Translation","type":"publication"},{"authors":["Chen Jia","Yue Zhang"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"0540913a0f97bfeaf058a281096b7e70","permalink":"https://westlake-nlp.github.io/publication/2022.-prompt-based-distribution-alignment-for-domain-generalization-in-text-classification/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2022.-prompt-based-distribution-alignment-for-domain-generalization-in-text-classification/","section":"publication","summary":"Prompt-based learning (a.k.a. prompting) achieves high performance by bridging the gap between the objectives of language modeling and downstream tasks. Domain generalization ability can be improved by prompting since classification across different domains can be unified into the prediction of the same set of label words. The remaining challenge for domain generalization by prompting comes from discrepancies between the data distribution of different domains. To improve domain generalization with prompting, we learn distributional invariance across source domains via two alignment regularization loss functions. The first is vocabulary distribution alignment, which uses a Kullback-Leibler divergence regularization on source-domain vocabulary distributions. The second is feature distribution alignment, which uses a novel adversarial training strategy to learn domain invariant representation across source domains. Experiments on sentiment analysis and natural language inference show the effectiveness of our method and achieve state-of-the-art results on six datasets.","tags":["Natural Language Processing"],"title":"Prompt-based Distribution Alignment for Domain Generalization in Text Classification","type":"publication"},{"authors":["Tengxiao Liu","Qipeng Guo","Xiangkun Hu","Yue Zhang","Xipeng Qiu","Zheng Zhang"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"7637c7458bb2cbadebb9390a512011e3","permalink":"https://westlake-nlp.github.io/publication/2022.-rlet-a-reinforcement-learning-based-approach-for-explainable-qa-with-entailment-trees/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2022.-rlet-a-reinforcement-learning-based-approach-for-explainable-qa-with-entailment-trees/","section":"publication","summary":"Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior single pass sequence-to-sequence models lack visible internal decision probability, while stepwise approaches are supervised with extracted single step data and cannot model the tree as a whole. In this work, we propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.","tags":["Reinforcement Learning","Natural Language Processing"],"title":"RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees","type":"publication"},{"authors":["Fangfang Su","Yue Zhang","Fei Li","Donghong Ji"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"12fc0ce7124e74405e4967a20a883e1b","permalink":"https://westlake-nlp.github.io/publication/2022.-balancing-precision-and-recall-for-neural-biomedical-event-extraction/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/2022.-balancing-precision-and-recall-for-neural-biomedical-event-extraction/","section":"publication","summary":"Biomedical event extraction is an essential task in the biomedical research. Existing models suffer from the issue of low recall due to the large proportion of unrecognized events and inflexible event argument combination. To address this issue, we propose an end-to-end multi-task approach for biomedical event extraction. Our model is able to achieve balanced precision and recall with several nichetargeting designs. First, neural encoders with rich lexical and syntactic features are used and shared by multiple subtasks such as event trigger recognition and argument relation extraction, in order to enhance the generalizability of the model. Second, a novel auxiliary subtask is added to identify the proteins that participate in the events, which helps decreasing the challenge of mining event-related proteins from the large candidate space. Third, event argument combination is performed using a strong neural network rather than inflexible rules or templates, to further increase the recall, especially for complex nested events. To demonstrate the effectiveness of our model, we evaluate it on two widely-used biomedical event extraction datasets used in the BioNLP 2011 and 2013 shared tasks. Our model achieves the state-of-the-art results (63.15% and 55.67% in F1 score) by significantly improving the recalls (compared with DeepEvnetMineSciBERT , 4.65% and 5.0%) on the two datasets. Further experiments and analyses show the effectiveness of our proposed features and modules in the model.","tags":["Natural Language Processing"],"title":"Balancing Precision and Recall for Neural Biomedical Event Extraction","type":"publication"},{"authors":["Leilei Gan","Zhiyang Teng","Yue Zhang","Linchao Zhu","Fei Wu","Yi Yang"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"2b5da695b8958dabf1745dc027dc7911","permalink":"https://westlake-nlp.github.io/publication/2022.-semglove-semantic-co-occurrences-for-glove-from-bert/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/2022.-semglove-semantic-co-occurrences-for-glove-from-bert/","section":"publication","summary":"GloVe learns word embeddings by leveraging statistical information from word co-occurrence matrices. However, word pairs in the matrices are extracted from a predefined local context window, which might lead to limited word pairs and potentially semantic irrelevant word pairs. In this paper, we propose SemGloVe, which distillssemantic co-occurrencesfrom BERT into static GloVe word embeddings. Particularly, we propose two models to extract co-occurrence statistics based on either the masked language model or the multi-head attention weights of BERT. Our methods can extract word pairs limited by the local window assumption, and can define the co-occurrence weights by directly considering the semantic distance between word pairs. Experiments on several word similarity datasets and external tasks show that SemGloVe can outperform GloVe.","tags":["Masked Language Model","Natural Language Processing"],"title":"SemGloVe: Semantic Co-occurrences for GloVe from BERT","type":"publication"},{"authors":["Chaoyi Huang","Wenyang Gao","Yingdie Zheng","Wei Wang","Yue Zhang","Kai Liu"],"categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"0356f98c94522c441339a6397da552bb","permalink":"https://westlake-nlp.github.io/publication/2022-universal-machine-learning-algorithm-for-predicting-adsorption-performance-of-organic-molecules-based-on-limited-data-set-importance-of-feature-description/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publication/2022-universal-machine-learning-algorithm-for-predicting-adsorption-performance-of-organic-molecules-based-on-limited-data-set-importance-of-feature-description/","section":"publication","summary":"Adsorption of organic molecules from aqueous solution offers a simple and effective method for their removal. Recently, there have been several attempts to apply machine learning (ML) for this problem. To this end, polyparameter linear free energy relationships (pp-LFERs) were employed, and poor prediction results were observed outside model applicability domain of pp-LFERs. In this study, we improved the applicability of ML methods by adopting a chemicalstructure (CS) based approach. We used the prediction of adsorption of organic molecules on carbon-based adsorbents as an example. Our results show that this approach can fully differentiate the structural differences between any organic molecules, while providing significant information that is relevant to their interaction with the adsorbents. We compared two CS feature descriptors: 3D-coordination and simplified molecular-input line-entry system (SMILES). We then built CS-ML models based on neural networks (NN) and extreme gradient boosting (XGB). They all outperformed pp-LFERs based models and are capable to accurately predict adsorption isotherm of isomers with similar physiochemical properties such as chiral molecules, even though they are trained with achiral molecules and racemates. We found for predicting adsorption isotherm, XGB shows better performance than NN, and 3D-coordinations allow effective differentiation between organic molecules.","tags":["Machine Learning"],"title":"Universal machine-learning algorithm for predicting adsorption performance of organic molecules based on limited data set: importance of feature description","type":"publication"},{"authors":["Yongjing Yin","Yafu Li","Fandong Meng","Jie Zhou","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"c7ff2a0ff5bd0722c617c7c949c5d967","permalink":"https://westlake-nlp.github.io/publication/2022.-categorizing-semantic-representations-for-neural-machine-translation/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-categorizing-semantic-representations-for-neural-machine-translation/","section":"publication","summary":"Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks. However, they have recently been shown to suffer limitation in compositional generalization, failing to effectively learn the translation of atoms (e.g., words) and their semantic composition (e.g., modification) from seen compounds (e.g., phrases), and thus suffering from significantly weakened translation performance on unseen compounds during inference.We address this issue by introducing categorization to the source contextualized representations. The main idea is to enhance generalization by reducing sparsity and overfitting, which is achieved by finding prototypes of token representations over the training set and integrating their embeddings into the source encoding. Experiments on a dedicated MT dataset (i.e., CoGnition) show that our method reduces compositional generalization error rates by 24% error reduction. In addition, our conceptually simple method gives consistently better results than the Transformer baseline on a range of general MT datasets.","tags":["Neural Machine Translation"],"title":"Categorizing Semantic Representations for Neural Machine Translation","type":"publication"},{"authors":["Yun Luo","Zihan Liu","Yuefeng Shi","Stan Z. Li","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"67d306465141538401c11a6957840f6e","permalink":"https://westlake-nlp.github.io/publication/2022.-exploiting-sentiment-and-common-sense-for-zero-shot-stance-detection/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-exploiting-sentiment-and-common-sense-for-zero-shot-stance-detection/","section":"publication","summary":"The stance detection task aims to classify the stance toward given documents and topics. Since the topics can be implicit in documents and unseen in training data for zero-shot settings, we propose to boost the transferability of the stance detection model by using sentiment and commonsense knowledge, which are seldom considered in previous studies. Our model includes a graph autoencoder module to obtain commonsense knowledge and a stance detection module with sentiment and commonsense. Experimental results show that our model outperforms the state-of-the-art methods on the zero-shot and few-shot benchmark dataset{--}VAST. Meanwhile, ablation studies prove the significance of each module in our model. Analysis of the relations between sentiment, common sense, and stance indicates the effectiveness of sentiment and common sense.","tags":["Sentiment Analysis"],"title":"Exploiting Sentiment and Common Sense for Zero-shot Stance Detection","type":"publication"},{"authors":["Yidong Wang","Hao Wu","Ao Liu","Wenxin Hou","Zhen Wu","Jindong Wang","Takahiro Shinozaki","Manabu Okumura","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"2dbbbd49962c438484205e8985f032d0","permalink":"https://westlake-nlp.github.io/publication/2022.-exploiting-unlabeled-data-for-target-oriented-opinion-words-extraction/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-exploiting-unlabeled-data-for-target-oriented-opinion-words-extraction/","section":"publication","summary":"Target-oriented Opinion Words Extraction (TOWE) is a fine-grained sentiment analysis task that aims to extract the corresponding opinion words of a given opinion target from the sentence. Recently, deep learning approaches have made remarkable progress on this task. Nevertheless, the TOWE task still suffers from the scarcity of training data due to the expensive data annotation process. Limited labeled data increase the risk of distribution shift between test data and training data. In this paper, we propose exploiting massive unlabeled data to reduce the risk by increasing the exposure of the model to varying distribution shifts. Specifically, we propose a novel Multi-Grained Consistency Regularization (MGCR) method to make use of unlabeled data and design two filters specifically for TOWE to filter noisy data at different granularity. Extensive experimental results on four TOWE benchmark datasets indicate the superiority of MGCR compared with current state-of-the-art methods. The in-depth analysis also demonstrates the effectiveness of the different-granularity filters.","tags":["Sentiment Analysis"],"title":"Exploiting Unlabeled Data for Target-Oriented Opinion Words Extraction","type":"publication"},{"authors":["Linyi Yang","Lifan Yuan","Leyang Cui","Wenyang Gao","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"74b8a9a51bc720693dc57de24dcb821b","permalink":"https://westlake-nlp.github.io/publication/2022.-factmix-using-a-few-labeled-in-domain-examples-to-generalize-to-cross-domain-named-entity-recognition/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-factmix-using-a-few-labeled-in-domain-examples-to-generalize-to-cross-domain-named-entity-recognition/","section":"publication","summary":"Few-shot Named Entity Recognition (NER) is imperative for entity tagging in limited resource domains and thus received proper attention in recent years. Existing approaches for few-shot NER are evaluated mainly under in-domain settings. In contrast, little is known about how these inherently faithful models perform in cross-domain NER using a few labeled in-domain examples. This paper proposes a two-step rationale-centric data augmentation method to improve the model's generalization ability. Results on several datasets show that our model-agnostic method significantly improves the performance of cross-domain NER tasks compared to previous state-of-the-art methods compared to the counterfactual data augmentation and prompt-tuning methods.","tags":["NER"],"title":"FactMix: Using a Few Labeled In-domain Examples to Generalize to Cross-domain Named Entity Recognition","type":"publication"},{"authors":["Yaoxian Song","Penglei Sun","Pengfei Fang","Linyi Yang","Yanghua Xiao","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"375228b3326c502a409868952c4516ba","permalink":"https://westlake-nlp.github.io/publication/2022.-human-in-the-loop-robotic-grasping-using-bert-scene-representation/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-human-in-the-loop-robotic-grasping-using-bert-scene-representation/","section":"publication","summary":"Current NLP techniques have been greatly applied in different domains. In this paper, we propose a human-in-the-loop framework for robotic grasping in cluttered scenes, investigating a language interface to the grasping process, which allows the user to intervene by natural language commands. This framework is constructed on a state-of-the-art grasping baseline, where we substitute a scene-graph representation with a text representation of the scene using BERT. Experiments on both simulation and physical robot show that the proposed method outperforms conventional object-agnostic and scene-graph based methods in the literature. In addition, we find that with human intervention, performance can be significantly improved. Our dataset and code are available on our project website https://sites.google.com/view/hitl-grasping-bert.","tags":["Human-in-the-loop"],"title":"Human-in-the-loop Robotic Grasping Using BERT Scene Representation","type":"publication"},{"authors":["Yun Luo","Fang Guo","Zihan Liu,","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"0cf37b11a25cb99278ab082a26da48d8","permalink":"https://westlake-nlp.github.io/publication/2022.-mere-contrastive-learning-for-cross-domain-sentiment-analysis/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-mere-contrastive-learning-for-cross-domain-sentiment-analysis/","section":"publication","summary":"Cross-domain sentiment analysis aims to predict the sentiment of texts in the target domain using the model trained on the source domain to cope with the scarcity of labeled data. Previous studies are mostly cross-entropy-based methods for the task, which suffer from instability and poor generalization. In this paper, we explore contrastive learning on the cross-domain sentiment analysis task. We propose a modified contrastive objective with in-batch negative samples so that the sentence representations from the same class can be pushed close while those from the different classes become further apart in the latent space. Experiments on two widely used datasets show that our model can achieve state-of-the-art performance in both cross-domain and multi-domain sentiment analysis tasks. Meanwhile, visualizations demonstrate the effectiveness of transferring knowledge learned in the source domain to the target domain and the adversarial test verifies the robustness of our model.","tags":["Sentiment Analysis","Contrastive Learning"],"title":"Mere Contrastive Learning for Cross-Domain Sentiment Analysis","type":"publication"},{"authors":["Chen Jia","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"d148b8cf9faa34bc7f6115c7e9a2a036","permalink":"https://westlake-nlp.github.io/publication/2022-metalearning-the-invariant-representation-for-domain-generalization/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022-metalearning-the-invariant-representation-for-domain-generalization/","section":"publication","summary":"Domain generalization studies how to generalize a machine learning model to unseen distributions. Learning invariant representation across different source distributions has been shown high effectiveness for domain generalization. However, the intrinsic possibility of overfitting in source domains can limit the generalization of invariance when faced with a target domain with large discrepancy to the source domains. To address this problem, we propose a meta-learning algorithm via bilevel optimization for domain generalization, where the inner-loop objective aims to minimize the discrepancy across different source domains while the outer-loop objective aims to minimize the discrepancy between source domains and a potential target domain. We show from a geometric perspective that the proposed algorithm can improve out-of-domain robustness for invariance learning. Empirically, we evaluate on five datasets and achieve the best results among a range of strong domain generalization baselines.","tags":["Machine Learning"],"title":"Meta-learning the invariant representation for domain generalization","type":"publication"},{"authors":["Zebin Ou","Meishan Zhang","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"72cec38c316a40e29685f7b371f43034","permalink":"https://westlake-nlp.github.io/publication/2022.-on-the-role-of-pre-trained-language-models-in-word-ordering-a-case-study-with-bart/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-on-the-role-of-pre-trained-language-models-in-word-ordering-a-case-study-with-bart/","section":"publication","summary":"Word ordering is a constrained language generation task taking unordered words as input. Existing work uses linear models and neural networks for the task, yet pre-trained language models have not been studied in word ordering, let alone why they help. We use BART as an instance and show its effectiveness in the task. To explain why BART helps word ordering, we extend analysis with probing and empirically identify that syntactic dependency knowledge in BART is a reliable explanation. We also report performance gains with BART in the related partial tree linearization task, which readily extends our analysis.","tags":["Pre-trained Language Model"],"title":"On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART","type":"publication"},{"authors":["Naihao Deng","Yulong Chen","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"79980e2564b171f1a2af8ae82e520446","permalink":"https://westlake-nlp.github.io/publication/2022.-recent-advances-in-text-to-sql-a-survey-of-what-we-have-and-what-we-expect/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-recent-advances-in-text-to-sql-a-survey-of-what-we-have-and-what-we-expect/","section":"publication","summary":"Text-to-SQL has attracted attention from both the natural language processing and database communities because of its ability to convert the semantics in natural language into SQL queries and its practical application in building natural language interfaces to database systems. The major challenges in text-to-SQL lie in encoding the meaning of natural utterances, decoding to SQL queries, and translating the semantics between these two forms. These challenges have been addressed to different extents by the recent advances. However, there is still a lack of comprehensive surveys for this task. To this end, we review recent progress on text-to-SQL for datasets, methods, and evaluation and provide this systematic survey, addressing the aforementioned challenges and discussing potential future directions. We hope this survey can serve as quick access to existing work and motivate future research.","tags":["Natural Language Processing"],"title":"Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect","type":"publication"},{"authors":["Xuefeng Bai","Linfeng Song","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"47f49bb5aae5352bcbc02a903575d9d4","permalink":"https://westlake-nlp.github.io/publication/2022.-semantic-based-pre-training-for-dialogue-understanding/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-semantic-based-pre-training-for-dialogue-understanding/","section":"publication","summary":"Pre-trained language models have made great progress on dialogue tasks. However, these models are typically trained on surface dialogue text, thus are proven to be weak in understanding the main semantic meaning of a dialogue context. We investigate Abstract Meaning Representation (AMR) as explicit semantic knowledge for pre-training models to capture the core semantic information in dialogues during pre-training. In particular, we propose a semantic-based pre-training framework that extends the standard pre-training framework (Devlin et al.,2019) by three tasks for learning 1) core semantic units, 2) semantic relations and 3) the overall semantic representation according to AMR graphs. Experiments on the understanding of both chit-chats and task-oriented dialogues show the superiority of our model. To our knowledge, we are the first to leverage a deep semantic representation for dialogue pre-training.","tags":["AMR"],"title":"Semantic-based Pre-training for Dialogue Understanding","type":"publication"},{"authors":["Kaixin Wu","Yue Zhang","Bojie Hu","Tong Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"4246d418d66f00ef4881842f1839b238","permalink":"https://westlake-nlp.github.io/publication/2022.-speeding-up-transformer-decoding-via-an-attention-refinement-network/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-speeding-up-transformer-decoding-via-an-attention-refinement-network/","section":"publication","summary":"Despite the revolutionary advances made by Transformer in Neural Machine Translation (NMT), inference efficiency remains an obstacle due to the heavy use of attention operations in auto-regressive decoding. We thereby propose a lightweight attention structure called Attention Refinement Network (ARN) for speeding up Transformer. Specifically, we design a weighted residual network, which reconstructs the attention by reusing the features across layers. To further improve the Transformer efficiency, we merge the self-attention and cross-attention components for parallel computing. Extensive experiments on ten WMT machine translation tasks show that the proposed model yields an average of 1.35x faster (with almost no decrease in BLEU) over the state-of-the-art inference implementation. Results on widely used WMT14 En-De machine translation tasks demonstrate that our model achieves a higher speed-up, giving highly competitive performance compared to AAN and SAN models with fewer parameter numbers.","tags":["Neural Machine Translation"],"title":"Speeding up Transformer Decoding via an Attention Refinement Network","type":"publication"},{"authors":["Yidong Wang","Hao Chen","Yue Fan","Wang Sun","Ran Tao","Wenxin Hou","Renjie Wang","Linyi Yang","Zhi Zhou","Lan-Zhe Guo","Heli Qi","Zhen Wu","Yu-Feng Li","Satoshi Nakamura","Wei Ye","Marios Savvides","Bhiksha Raj","Takahiro Shinozaki","Bernt Schiele","Jindong Wang","Xing Xie","Yue Zhang"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"b89ff87dd87a22ccd41fe20d5ef375a3","permalink":"https://westlake-nlp.github.io/publication/2022.-usb-a-unified-semi-supervised-learning-benchmark-for-classification/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/2022.-usb-a-unified-semi-supervised-learning-benchmark-for-classification/","section":"publication","summary":"Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pretrained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.","tags":["Datasets and Benchmarks"],"title":"USB: A Unified Semi-supervised Learning Benchmark for Classification","type":"publication"},{"authors":["Zhiyang Teng","Chenhua Chen","Yan Zhang","Yue Zhang"],"categories":null,"content":"","date":1659312e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312e3,"objectID":"be3fd83c93af2c1636b667becd42da56","permalink":"https://westlake-nlp.github.io/publication/2022.-contrastive-latent-variable-models-for-neural-text-generation/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/2022.-contrastive-latent-variable-models-for-neural-text-generation/","section":"publication","summary":"Deep latent variable models such as variational autoencoders and energy-based models are widely used for neural text generation. Most of them focus on matching the prior distribution with the posterior distribution of the latent variable for text reconstruction. In addition to instance-level reconstruction, this paper aims to integrate contrastive learning in the latent space, forcing the latent variables to learn high-level semantics by exploring inter-instance relationships. Experiments on various text generation benchmarks show the effectiveness of our proposed method. We also empirically show that our method can mitigate the posterior collapse issue for latent variable based text generation models.","tags":["Text Generation"],"title":"Contrastive Latent Variable Models for Neural Text Generation","type":"publication"},{"authors":["Shenghui Cheng","Yue Zhang","Xiaofei Li","Lin Yang","Xin Yuan","Stan Z. Li"],"categories":null,"content":"","date":1659312e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312e3,"objectID":"46d10b519bf1f1b897885faab15c0e9d","permalink":"https://westlake-nlp.github.io/publication/2022.-roadmap-toward-the-metaverse-an-ai-perspective/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/2022.-roadmap-toward-the-metaverse-an-ai-perspective/","section":"publication","summary":"Truth must be told, but not much‚Äîa hybrid society of real and virtual is coming. Metaverse, rise recently, is attracting significant attention from academia to industry. A metaverse is a network of three-dimensional (3D) virtual worlds focused on social connection. Bearing the outbreak of the coronavirus 2019 pandemic, people are physically isolated, which triggered the growth of the metaverse. Different from existing work, this commentary targets the roadmap of the metaverse from an artificial intelligence (AI) perspective","tags":["Artificial Intelligence"],"title":"Roadmap toward the metaverse: An AI perspective","type":"publication"},{"authors":["Chuang Fan","Daoxing Liu","Libo Qin,","Yue Zhang","Ruifeng Xu"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"c8d71b4338b9fdb4404d336d4c066cce","permalink":"https://westlake-nlp.github.io/publication/2022.-towards-event-level-causal-relation-identification/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/2022.-towards-event-level-causal-relation-identification/","section":"publication","summary":"Existing methods usually identify causal relations between events at the mention-level, which takes each event mention pair as a separate input. As a result, they either suffer from conflicts among causal relations predicted separately or require a set of additional constraints to resolve such conflicts. We propose to study this task in a more realistic setting, where event-level causality identification can be made. The advantage is two folds: 1) with modeling different mentions of an event as a single unit, no more conflicts among predicted results, without any extra constraints; 2) with the use of diverse knowledge sources (e.g., co-occurrence and coreference relations), a rich graph-based event structure can be induced from the document for supporting event-level causal inference. Graph convolutional network is used to encode such structural information, which aims to capture the local and non-local dependencies among nodes. Results show that our model achieves the best performance under both mention- and event-level settings, outperforming a number of strong baselines by at least 2.8% on F1 score.","tags":["Information Retrieval"],"title":"Towards Event-Level Causal Relation Identification","type":"publication"},{"authors":["Linyi Yang","Jiazheng Li","Ruihai Dong","Yue Zhang","Barry Smyth"],"categories":null,"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"5939fe367d436f3e329bde33abbd8bba","permalink":"https://westlake-nlp.github.io/publication/2022.-numhtml-numeric-oriented-hierarchical-transformer-model-for-multi-task-financial-forecasting/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/publication/2022.-numhtml-numeric-oriented-hierarchical-transformer-model-for-multi-task-financial-forecasting/","section":"publication","summary":"Financial forecasting has been an important and active area of machine learning research because of the challenges it presents and the potential rewards that even minor improvements in prediction accuracy or forecasting may entail. Traditionally, financial forecasting has heavily relied on quantitative indicators and metrics derived from structured financial statements. Earnings conference call data, including text and audio, is an important source of unstructured data that has been used for various prediction tasks using deep earning and related approaches. However, current deep learning-based methods are limited in the way that they deal with numeric data; numbers are typically treated as plain-text tokens without taking advantage of their underlying numeric structure. This paper describes a numeric-oriented hierarchical transformer model (NumHTML) to predict stock returns, and financial risk using multi-modal aligned earnings calls data by taking advantage of the different categories of numbers (monetary, temporal, percentages etc.) and their magnitude. We present the results of a comprehensive evaluation of NumHTML against several state-of-the-art baselines using a real-world publicly available dataset. The results indicate that NumHTML significantly outperforms the current state-of-the-art across a variety of evaluation metrics and that it has the potential to offer significant financial gains in a practical trading context.","tags":["Financial Forecasting"],"title":"NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-Task Financial Forecasting","type":"publication"},{"authors":["Li Du","Xiao Ding","Yue Zhang","Ting Liu","Bing Qin"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"302db6327e3eb896d19cb8eb62297b0a","permalink":"https://westlake-nlp.github.io/publication/2022.-a-graph-enhanced-bert-model-for-event-prediction/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-a-graph-enhanced-bert-model-for-event-prediction/","section":"publication","summary":"Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of relevant graph information, and hence influence the model performance. To address this issue, we consider automatically building of event graph using a BERT model. To this end, we incorporate an additional structured variable into BERT to learn to predict the event connections in the training process.Hence, in the test process, the connection relationship for unseen events can be predicted by the structured variable.Results on two event prediction tasks: script event prediction and story ending prediction, show that our approach can outperform state-of-the-art baseline methods.","tags":["Event Prediction"],"title":"A Graph Enhanced BERT Model for Event Prediction","type":"publication"},{"authors":["Jinghui Lu","Linyi Yang","Brian Mac Namee","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"547faeb7a76a19a9a35268892b822cca","permalink":"https://westlake-nlp.github.io/publication/2022.-a-rationale-centric-framework-for-human-in-the-loop-machine-learning/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-a-rationale-centric-framework-for-human-in-the-loop-machine-learning/","section":"publication","summary":"We present a novel rational-centric framework with human-in-the-loop Rationales-centric Double-robustness Learning (RDL) to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible inductive bias, exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.","tags":["Human-in-the-loop","Machine Learning"],"title":"A Rationale-Centric Framework for Human-in-the-loop Machine Learning","type":"publication"},{"authors":["Sen Yang","Leyang Cui","Ruoxi Ning","Di Wu","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"3c931692d3e328922fb60cf0ed854b5c","permalink":"https://westlake-nlp.github.io/publication/2022.-challenges-to-open-domain-constituency-parsing/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-challenges-to-open-domain-constituency-parsing/","section":"publication","summary":"Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We analyze challenges to open-domain constituency parsing using a set of linguistic features on various strong constituency parsers. Primarily, we find that 1) BERT significantly increases parsers' cross-domain performance by reducing their sensitivity on the domain-variant features.2) Compared with single metrics such as unigram distribution and OOV rate, challenges to open-domain constituency parsing arise from complex features, including cross-domain lexical and constituent structure variations.","tags":["Parsing"],"title":"Challenges to Open-Domain Constituency Parsing","type":"publication"},{"authors":["Chenhua Chen","Zhiyang Teng","Zhongqing Wang","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"d9e66209c7e80812ba92600afc791458","permalink":"https://westlake-nlp.github.io/publication/2022.-discrete-opinion-tree-induction-for-aspect-based-sentiment-analysis/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-discrete-opinion-tree-induction-for-aspect-based-sentiment-analysis/","section":"publication","summary":"Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification. Though being effective, such methods rely on external dependency parsers, which can be unavailable for low-resource languages or perform worse in low-resource domains. In addition, dependency trees are also not optimized for aspect-based sentiment classification. In this paper, we propose an aspect-specific and language-agnostic discrete latent opinion tree model as an alternative structure to explicit dependency trees. To ease the learning of complicated structured latent variables, we build a connection between aspect-to-context attention scores and syntactic distances, inducing trees from the attention scores. Results on six English benchmarks and one Chinese dataset show that our model can achieve competitive performance and interpretability.","tags":["Sentiment Analysis"],"title":"Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis","type":"publication"},{"authors":["Xuefeng Bai","Yulong Chen","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"8a76329a693f938379609d64b4bf5879","permalink":"https://westlake-nlp.github.io/publication/2022.-graph-pre-training-for-amr-parsing-and-generation/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-graph-pre-training-for-amr-parsing-and-generation/","section":"publication","summary":"Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure.Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively.However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs.In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training.We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks.Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model.To our knowledge, we are the first to consider pre-training on semantic graphs.","tags":["Parsing","AMR"],"title":"Graph Pre-training for AMR Parsing and Generation","type":"publication"},{"authors":["Leyang Cui","Sen Yang","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"0306303288dd337eaabadbaaa0b78f5d","permalink":"https://westlake-nlp.github.io/publication/2022.-investigating-non-local-features-for-neural-constituency-parsing/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-investigating-non-local-features-for-neural-constituency-parsing/","section":"publication","summary":"Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features. Recently, it has been shown that non-local features in CRF structures lead to improvements. In this paper, we investigate injecting non-local features into the training process of a local span-based parser, by predicting constituent n-gram non-local patterns and ensuring consistency between non-local patterns and local constituents. Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB. Besides, our method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and strong performance on CTB (92.31 F1). Our parser also outperforms the self-attentive parser in multi-lingual and zero-shot cross-domain settings.","tags":["Parsing"],"title":"Investigating Non-local Features for Neural Constituency Parsing","type":"publication"},{"authors":["Yafu Li","Yongjing Yin","Jing Li","Yue Zhang"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"0906fd0a9205f80f6039661c1d264142","permalink":"https://westlake-nlp.github.io/publication/2022.-prompt-driven-neural-machine-translation/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-prompt-driven-neural-machine-translation/","section":"publication","summary":"Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality. Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.","tags":["Neural machine translation"],"title":"Prompt-Driven Neural Machine Translation","type":"publication"},{"authors":["Jiangbin Zheng","Yile Wang","Ge Wang","Jun Xia","Yufei Huang","Guojiang Zhao","Yue Zhang","Stan Z. Li"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"42fd83f711fd86c7370b37400d31f076","permalink":"https://westlake-nlp.github.io/publication/2022.-using-context-to-vector-with-graph-retrofitting-to-improve-word-embeddings/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022.-using-context-to-vector-with-graph-retrofitting-to-improve-word-embeddings/","section":"publication","summary":"Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.","tags":["Natural Language Processing"],"title":"Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings","type":"publication"},{"authors":["Libo Qin","Minheng Ni","Yue Zhang","Wanxiang Che","Yangming Li","Ting Liu"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"3df1b1f3541d98b34ac6247724aebf47","permalink":"https://westlake-nlp.github.io/publication/2022.-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/2022.-multi-domain-spoken-language-understanding-using-domain-and-task-aware-parameterization/","section":"publication","summary":"Spoken language understanding (SLU) has been addressed as a supervised learning problem, where a set of training data is available for each domain. However, annotating data for a new domain can be both financially costly and non-scalable. One existing approach solves the problem by conducting multi-domain learning where parameters are shared for joint training across domains, which is domain-agnostic and task-agnostic. In the article, we propose to improve the parameterization of this method by using domain-specific and task-specific model parameters for fine-grained knowledge representation and transfer. Experiments on five domains show that our model is more effective for multi-domain SLU and obtain the best results. In addition, we show its transferability when adapting to a new domain with little data, outperforming the prior best model by 12.4%. Finally, we explore the strong pre-trained model in our framework and find that the contributions from our framework do not fully overlap with contextualized word representations (RoBERTa).","tags":["Natural Language Understanding"],"title":"Multi-Domain Spoken Language Understanding Using Domain- and Task-Aware Parameterization","type":"publication"},{"authors":["Âº†Â≤≥","Âê≥ÊÅ©ÈÅî"],"categories":null,"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1639353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"8809df18285b86c4992b99415da447d1","permalink":"https://westlake-nlp.github.io/news/getting-started/","publishdate":"2021-12-13T00:00:00Z","relpermalink":"/news/getting-started/","section":"news","summary":"**dsad** \nWelcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":null,"title":"Welcome to Wowchemy, the website builder for Hugo","type":"news"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"9ffdabb80e2a1e7ef0294aebd370da69","permalink":"https://westlake-nlp.github.io/news/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/news/writing-technical-content/","section":"news","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"news"},{"authors":["Âº†Â≤≥"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"942eddd08f7fe007d1b25aa44b6055f1","permalink":"https://westlake-nlp.github.io/news/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/news/jupyter/","section":"news","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"news"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ ( f\\left( x \\right) = ;\\frac{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}{\\left( {x + 4} \\right)\\left( {x + 1} \\right)} ) $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://westlake-nlp.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"111dsasdsaLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://westlake-nlp.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":null,"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://westlake-nlp.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":null,"title":"External Project","type":"project"},{"authors":["admindasda","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://westlake-nlp.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":" Updates Our class is now forwarded to ZhihuÔºåfeel free to follow us! New Lecture is online: Working with two texts [video] [slides] New Lecture is online: Neural structured prediction [video] [slides] New Lecture is online: Representation learning [video] [slides] New Assignment released: [Assignment #9 - chapter 9] New Lecture is online: Transition-based method for structure prediction [video] [slides] New Assignment released: [Assignment #8 - chapter 8] Course Description This online course is based on the recently released textbook, Natural Language Processing - a Machine Learning Perspective, written by Dr. Yue Zhang \u0026amp; Dr. Zhiyang Teng and published by Cambridge University Press. The course is taught in Chinese, English version of the class will be updated in future releases. Professor Yue Zhang is the principal inverstigator of the Text Intelligence Lab, Westlake University, and a well-known NLP researcher. You may find more detailed information at his personal page or Google Scholar. Course updates are in progress on our bilibili account, WestlakeNLP, do not hesitate to follow us! In this page, we will keep updating course slides.\nInstructors Zhang YueTenured Full Professor Teaching Assistants Chen Yulong Fu Qiankun Liu Hanmeng Liu Pai Materials Book Natural Language Processing - A Machine Learning Perspective\nAdditional Course Materials Other recomend textbooks and reading materials (update continuing). Material #1: Speech and Language processing Lectures You can download the lectures here. We will try to upload lectures prior to their corresponding classes. Date Title Material 09/08/2021 Wednesday Course Intro [notes] [video] [slides] 09/08/2021 Wednesday Content [notes] [video] [slides] 09/10/2021 Friday History of Natural Language Processing [notes] [video] [slides] 09/10/2021 Friday Counting relative frequencies [video] [slides] 10/13/2021 Wednesday Feature Vectors [video] [slides] 11/05/2021 Friday Discriminative Linear Classifiers [video] [slides] 12/06/2021 Monday Using Information Theory [video] [slides] 12/20/2021 Monday Hidden Variables [video] [slides] 01/07/2022 Friday Generative Sequence Labeling [video] [slides] 01/21/2022 Friday Discriminative Sequence Labeling [video] [slides] 03/07/2022 Monday Sequence Segmentation [video] [slides] 03/30/2022 Wednesday Neural Networks [video] [slides] 03/30/2022 Wednesday Neural Networks [video] [slides] 04/22/2022 Friday Predicting Tree Structures [video] [slides] 05/20/2022 Friday Transition-based method for structure prediction [video] [slides] 08/15/2022 Monday Representation learning [video] [slides] 09/05/2022 Monday Neural structured prediction [video] [slides] 09/05/2022 Monday Working with two texts [video] [slides] 12/05/2022 Monday Pre-training and Transfer Learning [video] Lectures List Assignments You can download the assignments here. Also check out each assignment page for any additional info.\nLate Policy You have free 8 late days. You can use late days for assignments. A late day extends the deadline 24 hours. Once you have used all 8 late days, the penalty is 10% for each additional late day. Release Date Description Due Date Material 03/05/2022 Saturday Assignment #1 - chapter 1 03/16/2022 23:59 Wednesday [problems] [solutions] 03/15/2022 Tuesday Assignment #2 - chapter 2 03/26/2022 23:59 Saturday [problems] [solutions] 04/05/2022 Tuesday Assignment #3 - chapter 3 04/16/2022 23:59 Saturday [problems] [solutions] 04/15/2022 Friday Assignment #4 - chapter 4 04/26/2022 23:59 Tuesday [problems] [solutions] 04/15/2022 Friday Assignment #5 - chapter 5 04/26/2022 23:59 Tuesday [problems] [solutions] 04/25/2022 Monday Assignment #6 - chapter 6 05/06/2022 23:59 Friday [problems] [solutions] 05/05/2022 Thursday Assignment #7 - chapter 7 05/16/2022 23:59 Monday [problems] [solutions] 05/15/2022 Sunday Assignment #8 - chapter 8 05/26/2022 23:59 Thursday [problems] [solutions] 05/25/2022 Wednesday Assignment #9 - chapter 9 06/06/2022 23:59 Monday [problems] [solutions] Assignments List ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8cfdf12688614f4b160275244f84de46","permalink":"https://westlake-nlp.github.io/teaching/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/example/","section":"teaching","summary":"**WestlakeNLP ¬∑ 2022**\n\nThis online course is based on the recently released textbook, Natural Language Processing - a Machine Learning Perspective, written by Dr. Yue Zhang \u0026 Dr. Zhiyang Teng and published by Cambridge University Press. The course is taught in Chinese, English version of the class will be updated in future releases. Professor Yue Zhang is the principal inverstigator of the Text Intelligence Lab, Westlake University, and a well-known NLP researcher. You may find more detailed information at his personal page or Google Scholar. Course updates are in progress on our bilibili account, WestlakeNLP, do not hesitate to follow us! In this page, we will keep updating course slides.","tags":["Natural Language Processing"],"title":"Natural Language Processing - A Machine Learning Perspective","type":"teaching"},{"authors":["WestlakeNLP"],"categories":null,"content":"Project List È°πÁõÆÂêçÁß∞ È°πÁõÆÁ±ªÂà´ ÂèÇ‰∏éËÄÖÔºà‰ªÖÁªüËÆ°Âú®ÂÆûÈ™åÂÆ§‰∫∫ÂëòÔºâ ËøõÂ∫¶ Êñ∞‰∏Ä‰ª£Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜË°®Á§∫ÊñπÊ≥ïÁ†îÁ©∂ ÊµôÊ±üÁúÅÁßëÊäÄÂéÖÈáçÁÇπÁ∫µÂêëÈ°πÁõÆ Êù®ÊûóÊòì„ÄÅÊùéÈõÖÂ§´„ÄÅÂ∞πÊ∞∏Á´û„ÄÅÁôΩÈõ™Â≥∞„ÄÅÁéãÂ≠òÁøî„ÄÅÁΩó‰∫ë„ÄÅÈóµÂ∫ÜÂáØ„ÄÅÈôàÈõ®Èæô„ÄÅÈÉ≠Êîæ„ÄÅÂàòÊ¥æ ËøõË°å‰∏≠ (2024-11-01) Âü∫‰∫éÊñ∞ÈóªÈ¢ÜÂüüÁöÑÂºÄÊîæÁõÆÊ†áÁ´ãÂú∫ËØÜÂà´ Êñ∞ÂçéÁ§æÊ®™ÂêëÈ°πÁõÆ Áü≥Â≤≥Â≥∞„ÄÅÁΩó‰∫ë„ÄÅÊù®ÊûóÊòì ËøõË°å‰∏≠ Âü∫‰∫é‰∫íËÅîÁΩëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂú®‰∏≠ÂõΩÂíåÂõΩÈôÖËµÑÊú¨Â∏ÇÂú∫ÁöÑÂàõÊñ∞Â∫îÁî® ÂõΩÂÆ∂Á∫ß‰∫∫ÊâçÈ°πÁõÆ Êù®ÊûóÊòì„ÄÅÈ©¨Ëã±Èπè ËøõË°å‰∏≠ (2023-12-31) ËØ≠Ë®ÄÊ°•Êú∫Âô®ÁøªËØëÁ≥ªÁªü ËØ≠Ë®ÄÊ°•Ê®™ÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ÔºåÈôàÈõ®ÈæôÔºåÂ∞πÊ∞∏Á´ûÔºåÊùéÈõÖÂ§´ÔºåÈ≤çÂÖâËÉúÔºåÈ¢úÂª∫Êòä ËøõË°å‰∏≠ (2025-12-31) Èù¢ÂêëÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁöÑË∑®È¢ÜÂüüÁü•ËØÜÊäΩÂèñ‰∏éËûçÂêà ÂõΩÂÆ∂Èù¢‰∏äÁ∫µÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ÔºåË¥æÊô® ËøõË°å‰∏≠ (2023-12-31) Ê®°ÂùóÂåñËØ≠‰πâÁ•ûÁªèÊú∫Âô®ÁøªËØëÁ†îÁ©∂‰∏éËêΩÂú∞ ÂõΩÂÆ∂ÁßëÊäÄÈÉ®ÂõΩÈôÖÂêà‰ΩúÊ∏ØÊæ≥Âè∞ÈáçÁÇπÁ∫µÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞„ÄÅÈ≤çÂÖâËÉú„ÄÅÊùéÈõÖÂ§´„ÄÅÂ∞πÊ∞∏Á´û„ÄÅÈ¢úÂª∫Êòä„ÄÅÊù®ÊûóÊòì ËøõË°å‰∏≠ (2025-12-31) Á•ûÁªèÂØπËØùÁ≥ªÁªüÁöÑÊ≥õÂåñÊÄßÁ†îÁ©∂ ËÖæËÆØÁäÄÁâõÈ∏üÊ®™ÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ ËøõË°å‰∏≠ ‰∏≠ÂõΩËµÑÊú¨Â∏ÇÂú∫ÊñáÊú¨Êô∫ËÉΩÁ†îÁ©∂ Âåó‰∫¨ËûçÊ±áÈáë‰ø°Ê®™ÂêëÈ°πÁõÆ Êù®ÊûóÊòì ËøõË°å‰∏≠ ‰æµÂÖ•ÂºèËØ≠Èü≥Ëß£Á†Å ÊµôÂ§ß‰∫åÈô¢Ê®™ÂêëÈ°πÁõÆ ÊõπË∑Ø„ÄÅÂÜØÁêõ„ÄÅÊ¨ßÊ≥ΩÊñå ËøõË°å‰∏≠ Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàêÁöÑÂâçÊ≤øÊäÄÊúØ‰∏éÂ∫îÁî® ËÖæËÆØÂæÆ‰ø°Ê®™ÂêëÈ°πÁõÆ Â∞πÊ∞∏Á´û ËøõË°å‰∏≠ ÊèèËø∞ÊÄßÈïøÊñáÊú¨ÁöÑËá™Âä®ÁêÜËß£‰∏éÁîüÊàêÊäÄÊúØÁ†îÁ©∂ ÂåóÂ§ßÈù¢‰∏äÁ∫µÂêëÂêà‰ΩúÈ°πÁõÆ È≤çÂÖâËÉú„ÄÅËÉ°Ê∂µÊó≠„ÄÅÈôàÈõ®Èæô ËøõË°å‰∏≠ ‰πãÊ±üÂÆûÈ™åÂÆ§‰∫ã‰ª∂ÊäΩÂèñ ‰πãÊ±üÂÆûÈ™åÂÆ§Á∫µÂêëÈ°πÁõÆ ÈóµÂ∫ÜÂáØ„ÄÅÈ´òÊñáÁÇÄ„ÄÅÂàòÊ¥æ„ÄÅÊ†óËã±Êù∞ Áî≥ËØ∑‰∏≠ Project List ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://westlake-nlp.github.io/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Project List È°πÁõÆÂêçÁß∞ È°πÁõÆÁ±ªÂà´ ÂèÇ‰∏éËÄÖÔºà‰ªÖÁªüËÆ°Âú®ÂÆûÈ™åÂÆ§‰∫∫ÂëòÔºâ ËøõÂ∫¶ Êñ∞‰∏Ä‰ª£Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜË°®Á§∫ÊñπÊ≥ïÁ†îÁ©∂ ÊµôÊ±üÁúÅÁßëÊäÄÂéÖÈáçÁÇπÁ∫µÂêëÈ°πÁõÆ Êù®ÊûóÊòì„ÄÅÊùéÈõÖÂ§´„ÄÅÂ∞πÊ∞∏Á´û„ÄÅÁôΩÈõ™Â≥∞„ÄÅÁéãÂ≠òÁøî„ÄÅÁΩó‰∫ë„ÄÅÈóµÂ∫ÜÂáØ„ÄÅÈôàÈõ®Èæô„ÄÅÈÉ≠Êîæ„ÄÅÂàòÊ¥æ ËøõË°å‰∏≠ (2024-11-01) Âü∫‰∫éÊñ∞ÈóªÈ¢ÜÂüüÁöÑÂºÄÊîæÁõÆÊ†áÁ´ãÂú∫ËØÜÂà´ Êñ∞ÂçéÁ§æÊ®™ÂêëÈ°πÁõÆ Áü≥Â≤≥Â≥∞„ÄÅÁΩó‰∫ë„ÄÅÊù®ÊûóÊòì ËøõË°å‰∏≠ Âü∫‰∫é‰∫íËÅîÁΩëÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂú®‰∏≠ÂõΩÂíåÂõΩÈôÖËµÑÊú¨Â∏ÇÂú∫ÁöÑÂàõÊñ∞Â∫îÁî® ÂõΩÂÆ∂Á∫ß‰∫∫ÊâçÈ°πÁõÆ Êù®ÊûóÊòì„ÄÅÈ©¨Ëã±Èπè ËøõË°å‰∏≠ (2023-12-31) ËØ≠Ë®ÄÊ°•Êú∫Âô®ÁøªËØëÁ≥ªÁªü ËØ≠Ë®ÄÊ°•Ê®™ÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ÔºåÈôàÈõ®ÈæôÔºåÂ∞πÊ∞∏Á´ûÔºåÊùéÈõÖÂ§´ÔºåÈ≤çÂÖâËÉúÔºåÈ¢úÂª∫Êòä ËøõË°å‰∏≠ (2025-12-31) Èù¢ÂêëÊàêÂàÜÂè•Ê≥ïÂàÜÊûêÁöÑË∑®È¢ÜÂüüÁü•ËØÜÊäΩÂèñ‰∏éËûçÂêà ÂõΩÂÆ∂Èù¢‰∏äÁ∫µÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ÔºåË¥æÊô® ËøõË°å‰∏≠ (2023-12-31) Ê®°ÂùóÂåñËØ≠‰πâÁ•ûÁªèÊú∫Âô®ÁøªËØëÁ†îÁ©∂‰∏éËêΩÂú∞ ÂõΩÂÆ∂ÁßëÊäÄÈÉ®ÂõΩÈôÖÂêà‰ΩúÊ∏ØÊæ≥Âè∞ÈáçÁÇπÁ∫µÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞„ÄÅÈ≤çÂÖâËÉú„ÄÅÊùéÈõÖÂ§´„ÄÅÂ∞πÊ∞∏Á´û„ÄÅÈ¢úÂª∫Êòä„ÄÅÊù®ÊûóÊòì ËøõË°å‰∏≠ (2025-12-31) Á•ûÁªèÂØπËØùÁ≥ªÁªüÁöÑÊ≥õÂåñÊÄßÁ†îÁ©∂ ËÖæËÆØÁäÄÁâõÈ∏üÊ®™ÂêëÈ°πÁõÆ ÁôΩÈõ™Â≥∞ ËøõË°å‰∏≠ ‰∏≠ÂõΩËµÑÊú¨Â∏ÇÂú∫ÊñáÊú¨Êô∫ËÉΩÁ†îÁ©∂ Âåó‰∫¨ËûçÊ±áÈáë‰ø°Ê®™ÂêëÈ°πÁõÆ Êù®ÊûóÊòì ËøõË°å‰∏≠ ‰æµÂÖ•ÂºèËØ≠Èü≥Ëß£Á†Å ÊµôÂ§ß‰∫åÈô¢Ê®™ÂêëÈ°πÁõÆ ÊõπË∑Ø„ÄÅÂÜØÁêõ„ÄÅÊ¨ßÊ≥ΩÊñå ËøõË°å‰∏≠ Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàêÁöÑÂâçÊ≤øÊäÄÊúØ‰∏éÂ∫îÁî® ËÖæËÆØÂæÆ‰ø°Ê®™ÂêëÈ°πÁõÆ Â∞πÊ∞∏Á´û ËøõË°å‰∏≠ ÊèèËø∞ÊÄßÈïøÊñáÊú¨ÁöÑËá™Âä®ÁêÜËß£‰∏éÁîüÊàêÊäÄÊúØÁ†îÁ©∂ ÂåóÂ§ßÈù¢‰∏äÁ∫µÂêëÂêà‰ΩúÈ°πÁõÆ È≤çÂÖâËÉú„ÄÅËÉ°Ê∂µÊó≠„ÄÅÈôàÈõ®Èæô ËøõË°å‰∏≠ ‰πãÊ±üÂÆûÈ™åÂÆ§‰∫ã‰ª∂ÊäΩÂèñ ‰πãÊ±üÂÆûÈ™åÂÆ§Á∫µÂêëÈ°πÁõÆ ÈóµÂ∫ÜÂáØ„ÄÅÈ´òÊñáÁÇÄ„ÄÅÂàòÊ¥æ„ÄÅÊ†óËã±Êù∞ Áî≥ËØ∑‰∏≠ Project List ","tags":null,"title":"Projects","type":"page"},{"authors":["WestlakeNLP"],"categories":null,"content":"Reading List 2022 Title Date Time Presentor Resource CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior 2022/9/14 2:00-3:00pm Hongliang Li Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems 2022/9/21 2:00-3:00pm Chiyu Song CausaLM: Causal Model Explanation through Counterfactual Language Models 2022/9/29 3:30-4:30pm Linyi Yang AI and the Everything in the Whole Wide World Benchmark 2022/9/29 3:30-4:30pm Naihao Deng Bias Mitigation in Machine Translation Quality Estimation 2022/10/10 1:30-2:30pm Yafu Li Discussion for AI and the Everything in the Whole Wide World Benchmark 2022/10/10 2:00-3:00pm TableFormer: Robust Transformer Modeling for Table-Text Encoding 2022/10/19 2:00-3:00pm Naihao Deng [Github] Self-explaining deep models with logic rule reasoning 2022/10/26 2:00-4:00pm Linyi Yang [Microsoft Research] [Code] Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation 2022/10/26 2:00-4:00pm Jianhao Yan Better Language Model with Hypernym Class Prediction 2022/10/26 2:00-4:00pm Guangsheng Bao Open Domain Question Answering with A Unified Knowledge Interface 2022/11/2 2:00-4:00pm Cunxiang Wang and Naihao Deng History of Philosophy of Language: Does There Exist a Parallelism to the History of NLP 2022/11/9 2:00-3:00pm Ruoxi Ning Meta-learning via Language Model In-context Tuning 2022/11/16 2:00-4:00pm Naihao Deng Consensuses and disagreements in the in-context learning studies 2022/11/23 2:00-3:00pm Hongliang Li 2022/11/30 2:00-3:00pm MACSUM: Controllable Summarization with Mixed Attributes 2022/12/7 2:00-4:00pm Yulong Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks 2022/12/7 2:00-4:00pm Xuefeng A Meta-framework for Spatiotemporal Quantity Extraction from Text 2022/12/7 2:00-4:00pm Fang BRIO: Bringing Order to Abstractive Summarization 2022/12/14 2:00-4:00pm Guangsheng Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data 2022/12/14 2:00-4:00pm Cunxiang 2022/12/21 2:00-4:00pm 2022/12/21 2:00-4:00pm 2022/12/28 2:00-4:00pm 2023/1/11 2:00-4:00pm 2023/1/11 2:00-4:00pm 2023/1/18 2:00-4:00pm Reading List 2022 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6da302e29c14c780e10955c05f7c11ad","permalink":"https://westlake-nlp.github.io/reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reading/","section":"","summary":"Reading List 2022 Title Date Time Presentor Resource CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior 2022/9/14 2:00-3:00pm Hongliang Li Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems 2022/9/21 2:00-3:00pm Chiyu Song CausaLM: Causal Model Explanation through Counterfactual Language Models 2022/9/29 3:30-4:30pm Linyi Yang AI and the Everything in the Whole Wide World Benchmark 2022/9/29 3:30-4:30pm Naihao Deng Bias Mitigation in Machine Translation Quality Estimation 2022/10/10 1:30-2:30pm Yafu Li Discussion for AI and the Everything in the Whole Wide World Benchmark 2022/10/10 2:00-3:00pm TableFormer: Robust Transformer Modeling for Table-Text Encoding 2022/10/19 2:00-3:00pm Naihao Deng [Github] Self-explaining deep models with logic rule reasoning 2022/10/26 2:00-4:00pm Linyi Yang [Microsoft Research] [Code] Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation 2022/10/26 2:00-4:00pm Jianhao Yan Better Language Model with Hypernym Class Prediction 2022/10/26 2:00-4:00pm Guangsheng Bao Open Domain Question Answering with A Unified Knowledge Interface 2022/11/2 2:00-4:00pm Cunxiang Wang and Naihao Deng History of Philosophy of Language: Does There Exist a Parallelism to the History of NLP 2022/11/9 2:00-3:00pm Ruoxi Ning Meta-learning via Language Model In-context Tuning 2022/11/16 2:00-4:00pm Naihao Deng Consensuses and disagreements in the in-context learning studies 2022/11/23 2:00-3:00pm Hongliang Li 2022/11/30 2:00-3:00pm MACSUM: Controllable Summarization with Mixed Attributes 2022/12/7 2:00-4:00pm Yulong Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks 2022/12/7 2:00-4:00pm Xuefeng A Meta-framework for Spatiotemporal Quantity Extraction from Text 2022/12/7 2:00-4:00pm Fang BRIO: Bringing Order to Abstractive Summarization 2022/12/14 2:00-4:00pm Guangsheng Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data 2022/12/14 2:00-4:00pm Cunxiang 2022/12/21 2:00-4:00pm 2022/12/21 2:00-4:00pm 2022/12/28 2:00-4:00pm 2023/1/11 2:00-4:00pm 2023/1/11 2:00-4:00pm 2023/1/18 2:00-4:00pm Reading List 2022 ","tags":null,"title":"Reading Group","type":"page"},{"authors":["WestLakeNLP"],"categories":null,"content":"Conference Tutorials Year Conference Title 2022 North American Chapter of the Association for Computational Linguistics (NAACL) Contrastive learning for NLP 2021 ADL.Chinese Computer Federation (CCF) Graph Neural Networks in NLP 2019 ATT14.Chinese Information Processing Society of China (CIPS) Graph Neural Networks in NLP 2018 The Empirical Methods on Natural Language Processing (EMNLP) Joint Models in NLP 2017 The International Joint Conference on Natural Language Processing (IJCNLP) Deep Learning for Syntactic and Semantic Analysis 2017 The China National Conference on Computational Linguistics (CCL) Joint Models for NLP 2016 The Empirical Methods for Natural Language Processing (EMNLP) Neural Network for Sentiment Analysis 2016 The China National Conference on Computational Linguistics (CCL) Deep Learning for Syntactic and Semantic Analysis 2014 The Annual Meeting of the Association for Computational Linguistics (ACL) Syntactic Processing Using Global Discriminative Learning and Beam-Search Decoding 2010 The Annual Conference ofthe North American Chapter of the Association for Computational Linguistics (HLT:NAACL) Recent Advances in Dependency Parsing Conference Tutorials Invited Talks Year Talk Title 2022 Keynote at MIT Technology Review 20 year forum Robustness for machine translation 2022 Keynote at CNCC few-shot NLP forum Data Augmentation for few-shot NLI 2022 Keynote at CCAC Semantic structures for sentiment 2022 University of Edinburgh ICCS OOD generalization for NLP 2021 Keynote at IALP Challenges in NLP 2021 Keynote at PACLIC Document-Level NLP 2021 Oxford ML Summer School Challenges in NLP 2021 Keynote at CAAI Open-domain Dialogue 2021 Keynote at YSSNLP Compositional Generalization of Machine Translation 2020 Keynote at CTC Beyond Words ‚Äî Commonsense Reasoning in Language Understanding 2020 Chair at COLING2020 PANEL2 On The Generalization of NLP Models ‚Äì Ways Around Data Hunger 2020 Keynote at COLING2020 WS-MSR AMR to Text Generation - A Brief Review And A Case Study Using Back-parsing 2020 Keynote at CAAI Industry Forum The Bottleneck of NLP Fundamental Tasks in NLP 2019 Hundsun Technologies Inc Natural Language Processing And Computational Financial Research 2019 Technische Universit Ãàat Darmstadt Graph Neural Networks in NLP 2019 HSBC open day on Sep NLP and financial markets 2019 Keynote at CCF Social Media Program Cross-domain sentiment analysis 2019 Dagstuhl seminar Invited attendance 2018 Uppsala University Graph Recurrent Neural Networks 2018 Cloud Town Conference by Alibaba Deep Learning for parsing 2017 Yuan Ze University Stock Prediction 2017 A-STAR Deep Learning for parsing 2017 Keynote at Singapore Deep Learning for Finance Summit Using deep learning for predicting USA stock markets 2016 Keynote at Deep Learning Summit Singapore Deep Learning for sentiment analysis 2016 Keynote at SmartConnected.World and ICEC 2016. Suwon. Korea Deep Learning 2016 University of Berkeley Neural network structured prediction 2016 University of South California. Information Science Institute Neural network structured prediction 2016 Singapore Actuary Society Text mining 2015 Keynote at Global Innovators Forum on Technology and Society. Harbin. China Text mining 2015 Keynote at ADSM Deep learning workshop Deep Learning for stock market prediction 2014 Tsinghua University (China) CCG parsing 2014 Universitat Pompeu Fabra (Spain) Statistical text generation 2014 University of A Coruna (Spain) Chinese syntactic analysis 2014 Peking University Chinese syntactic analysis 2013 Tsinghua University Dependency parsing 2013 Baidu Inc (China) Word ordering 2013 Keynote at CWMT Semantics Based Statistical Machine Translation 2012 Institute of Infocomm Research (Singapore) CCG parsing 2011 Cambridge NLIP seminar Surprisingly Efficient Parsing for a Wide-Coverage Lexicalised-Grammar Parser 2010 Uppsala University (Sweden) Dependency parsing 2009 the NLIP group seminar. University of Cambridge Dependency parsing 2008 the School of Informatics (ICCS). University of Edinburgh Dependency parsing 2008 Sharp Research Europe Ltd. Word segmentation Invited Talks ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://westlake-nlp.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Conference Tutorials Year Conference Title 2022 North American Chapter of the Association for Computational Linguistics (NAACL) Contrastive learning for NLP 2021 ADL.Chinese Computer Federation (CCF) Graph Neural Networks in NLP 2019 ATT14.","tags":null,"title":"Research Seminars","type":"page"}]